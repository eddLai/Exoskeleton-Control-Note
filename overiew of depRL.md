---
bg: "[[NTKLab_white bg.png]]"
---

<style>
    .reveal {
        font-family: 'Times New Roman', 'æ¨™æ¥·é«”';
        font-size: 30px;
        text-align: left;
        color: black;
        background-size: cover;
        background-position: center;
    }
	.reveal h1,
	.reveal h2,
	.reveal h3,
	.reveal h4,
	.reveal h5,
	.reveal h6 {
	  font-family: 'Times New Roman', 'æ¨™æ¥·é«”';
	  color: black;
	  %%text-transform: lowercase%%;
	  text-transform: capitalize;
	}
	.with-border{
		border: 1px solid red;
	}
</style>
<grid drag="60 10" drop="-3 40">
RL utility on Muscleskeleton modeling
<!-- element style="font-size: 35px;align: left; text-align: left;color: white"-->
</grid>

<grid drag="50 10" drop="40 70">
è³´å®é”ã€åŠ‰æ™ºç¿”
<!-- element style="font-size: 40px;align: right; text-align: right"-->
</grid>

<!-- slide bg="../NTKLab_white bg_cover_resize.png"-->

---
[[Natural and Robust Walking using Reinforcement Learning without Demonstrations in High-Dimensional Musculoskeletal Models.pdf]]
[[DEP-RL EMBODIED EXPLORATION FOR REINFORCEMENT LEARNING IN OVERACTUATED AND MUSCULOSKELETAL SYSTEMS.pdf]]
# Background
training file
settings file
desired algorithm, environment, number of training iterations

## Terms
- embodimentï¼šå¯¦ç¾éƒ¨ä»¶
- Differential Extrinsic Plasticity, DEPï¼šè‡ªçµ„ç¹”ä¸¦ç”¢ç”Ÿé€£è²«çš„è¡Œç‚º
- evolutionary priorsï¼šå…§å»ºåœ¨ç”Ÿç‰©é«”å…§çš„æ©Ÿåˆ¶æˆ–ç›®æ¨™

D4PGå°æ–¼å–®ä¸€ä»»å‹™çš„å¤šDOFå‹•ä½œæ‡‰è©²è¶³å¤ 

---
## primary challenge
è‚Œè‚‰æœ¬èº«çš„åŒ–å­¸ä½é€šæ¿¾æ³¢ç‰¹æ€§ï¼Œå–®æ¬¡çŸ­æš«çš„è‚Œè‚‰æ”¶ç¸®æ‰€ç”¢ç”Ÿçš„åŠ›çŸ©ï¼Œé€šå¸¸ä¸è¶³ä»¥åœ¨é—œç¯€å±¤é¢ç”¢ç”Ÿè¶³å¤ çš„é‹å‹•
>***"torques generated by short muscle twitches are often not sufficient to induce adequate motions on the joint level due to chemical low-pass filter properties"***

### Abstraction of muscle control
- motion primitives: é€éé è™•ç†ï¼Œå°‡è¨Šè™Ÿé™ç¶­ï¼Œextraction of muscle synergies
- é—œç¯€åŠ›çŸ© (joint torques) è½‰æ›ç‚ºè‚Œè‚‰åˆºæ¿€ (muscle stimulations) çš„æ˜ å°„é—œä¿‚

å¾åŸºæœ¬ç”Ÿç†æ©Ÿåˆ¶è¨­è¨ˆçå‹µå‡½æ•¸ï¼š
- è¡Œèµ°é€Ÿåº¦
- é—œç¯€ç–¼ç—›
- è‚Œè‚‰åŠªåŠ›

---
## Challenge to the traditional RL
ä½¿ç”¨RL methodsï¼Œéœ€è¦æ³¨æ„ï¼š
- éåº¦é©…å‹•ç³»çµ± (Overactuated Systems): ç”Ÿç‰©è‚Œè‚‰éª¨éª¼ç³»çµ±é€šå¸¸æ˜¯éåº¦é©…å‹•çš„ï¼Œå³è‚Œè‚‰æ•¸é‡å¤šæ–¼è‡ªç”±åº¦ã€‚åœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œé«˜æ•ˆçš„æ¢ç´¢ç­–ç•¥æ˜¯è‡³é—œé‡è¦çš„ã€‚
- å‹•ä½œç©ºé–“ (Action Space): è‚Œè‚‰æ§åˆ¶ä»»å‹™çš„å‹•ä½œç©ºé–“é€šå¸¸å¾ˆå¤§ï¼Œå› ç‚ºéœ€è¦æ§åˆ¶è¨±å¤šå–®ç¨çš„è‚Œè‚‰
- Îµ-greedy ç­–ç•¥æˆ–é›¶å‡å€¼ä¸ç›¸é—œé«˜æ–¯é›œè¨Šä¸å¤ ç”¨
- Ornstein-Uhlenbeck é›œè¨Šã€‚å°æ–¼è·¨å‹•ä½œç›¸é—œæ€§çš„å¾é ­é–‹å§‹å­¸ç¿’ä»»å‹™ä¸å¤ 
- é›£ä»¥é€šéç°¡å–®çš„è©¦éŒ¯æ³•æ¢ç´¢

ä¸€æ¨£è¦å…ˆæ“¬åˆå‡ºé€™å€‹äººçš„è‚Œè‚‰ç‹€æ…‹(è¦æ€éº¼å°å…¥opensimè³‡æ–™)ï¼Œç„¶å¾Œå†é€éRLä¾†æ‡‰å°è·Œå€’ä¹‹é¡çš„å•é¡Œ
- æ˜ç¢ºçš„çå‹µå¡‘å½¢
- ç¤ºç¯„å­¸ç¿’
- ç‰¹å®šè¨“ç·´èª²ç¨‹èˆ‡æª¢æŸ¥é»

---
# Literature Review
- é™ä½è‚Œè‚‰ç¶­åº¦
	- Tieck et al., 2018
	- Tahami et al., 2014
	- Crowder et al., 2021
- æ‰‹å‹•åˆ†çµ„è‚Œè‚‰ä»¥ç°¡åŒ–å­¸ç¿’
	- Joos et al., 2020
- é€è»Œè·¡æœ€ä½³åŒ–æ§åˆ¶ (Al Borno et al., 2020)
- ä½¿ç”¨å‹•ä½œæ•æ‰æ•¸æ“š (Lee et al., 2019)
- åœ¨çœŸå¯¦çš„è‚Œè‚‰æ©Ÿå™¨äººä¸Šç”¢ç”Ÿå‹•ä½œ (Driess et al., 2018; Buchler et al., 2016)
- æ˜ç¢ºçš„çå‹µå¡‘å½¢ã€ç¤ºç¯„å­¸ç¿’ã€ç‰¹å®šè¨“ç·´trialsèˆ‡æª¢æŸ¥é»NeurIPS
- Large Action Spaces
	- Hypergraph-based Architecture:Tavakoli et al., 2021
	- é›¢æ•£å‹•ä½œç©ºé–“ Dulac-Arnold et al., 2016; Wang et al., 2016
	- è¡Œå‹•å…ˆé©— (Action Priors) Biza et al., 2021; Singh et al., 2021å°ˆå®¶æ•¸æ“š
- è¡Œå‹•ç©ºé–“ç°¡åŒ–èˆ‡è‚Œè‚‰æ§åˆ¶
	- PDæ§åˆ¶å™¨Luo et al., 2021
	- æ ¡æ­£ç¶²è·¯é™åˆ¶åŠ›çŸ©è‡´å‹•å™¨çš„è¼¸å‡ºJiang et al., 2019
	- PCAé™ç¶­ï¼šAl Borno et al., 2020; Zhao et al., 2022

---
## Hebbian learning
HLï¼šæ´»èºï¼Œå¢å¼·é€£çµ(æ„Ÿæ¸¬å™¨å’Œè‡´å‹•å™¨çš„æ§åˆ¶ç¶²è·¯)ï¼Œå…§éƒ¨è€Œéå¤–éƒ¨äº’å‹•
***"Initial exploration As DEP is creating exploration that excites the system into various modes, we suggest running an unsupervised pre-training phase with exclusive DEP control."***
æ§åˆ¶å™¨å‹•ä½œ
$$a_t = \tanh(\kappa C s_t + h_t)$$
- Cï¼šæ§åˆ¶çŸ©é™£ï¼Œç‹€æ…‹èˆ‡å‹•ä½œé–“çš„mapping
- hï¼šä½œç‚ºåŸºæº–å€¼ï¼Œ(joint limit) $\dot{h} \propto -a_t$
- é€éæ­¤å‡¸é¡¯é€£çµ$$C_{ij} = a_{i,\ t} \cdot s_{j,\ t}$$
- å·®åˆ†HL$$C_{ij} = a_{i,\ t} \cdot \Delta s_{j,\ t}$$
ä»¥ä¸Šè§€æ¸¬ç¯„åœå¤ªå°äº†ã€‚
## Differential Extrinsic Plasticity(a learning rule)
DEPåŠ å…¥äº†é•·æœŸç’°å¢ƒå›é¥‹ï¼Œå³ä½¿ç”¨ç‹€æ…‹é€Ÿåº¦(ç‹€æ…‹çš„æ™‚è®Šé‡)ï¼Œç”¨æ–¼åŠ å¼·ç‹€æ…‹ç©ºé–“ä¸­é€£è²«é‹å‹•çš„æ„Ÿæ¸¬å™¨å’Œå‹•ä½œä¹‹é–“çš„é€£æ¥ã€‚

$$\tau \dot{C} = f(\dot{s}_t) \dot{s}_{t-\Delta t}^T - C$$
>recallï¼šä¸€éšè¡°æ¸›é …ï¼Œéš¨æ™‚é–“é€æ¼¸æ¸›å¼±
>inverse prediction model$f(a)$å¯ç°¡åŒ–ç‚ºa
>å¯ä»¥å‡è¨­æ„Ÿæ¸¬å™¨ï¼ˆsensorï¼‰èˆ‡åŸ·è¡Œå™¨ï¼ˆactuatorï¼‰è¿‘ä¼¼ç·šæ€§ç­‰ä¹‹é¡çš„

ç‹€æ…‹ç©ºé–“çš„æ´»å‹•æ€§
$$\tilde{C}_{ij} = \frac{C_{ij}}{\|C_{ij}\|_i + \epsilon}$$

### å•é¡Œ
- perturbations sensitive
- behavioral deprivation: ä¾·é™åœ¨ç‰¹å®šæ¨¡å¼
åœ¨é«˜ç¶­ä¸­å……åˆ†stochasticityå¯ä»¥é¿å…
- DEP èˆ‡ RL çµåˆçš„å„ªå‹¢ï¼šå¼·åŒ–å­¸ç¿’ç­–ç•¥å¯ä»¥å¹²é  DEPï¼Œé¿å…å…¶é™·å…¥ç‰¹å®šçš„æ¥µé™é€±æœŸï¼Œå¢å¼·å­¸ç¿’çš„éˆæ´»æ€§ã€‚
- DEP çš„ç›´è¦ºï¼šDEP èƒ½å¤ å¿«é€Ÿå»ºç«‹ç‹€æ…‹èˆ‡å‹•ä½œä¹‹é–“çš„é«˜åº¦ç›¸é—œæ€§ï¼Œå¯¦ç¾ç³»çµ±çš„å”èª¿é‹å‹•ã€‚

---
# Workflow
1. åˆå§‹æ¢ç´¢ï¼ˆInitial Explorationï¼‰ï¼š
å…ˆç”± DEP é€²è¡Œå®Œå…¨æ§åˆ¶çš„æ¢ç´¢ï¼Œæ”¶é›†å¤§é‡éç›£ç£æ•¸æ“šï¼Œåˆå§‹åŒ–ç·©è¡å€ã€‚

2. äº¤æ›¿æ§åˆ¶ï¼ˆIntra-Episode Explorationï¼‰ï¼š
	- åœ¨æ¯å€‹ episode ä¸­ï¼ŒDEP å’Œ RL ç­–ç•¥äº¤æ›¿æ§åˆ¶ç³»çµ±ï¼š
		- RL ç­–ç•¥è² è²¬ç›®æ¨™å°å‘çš„è¡Œç‚ºã€‚
		- DEP ç­–ç•¥è² è²¬éš¨æ©Ÿæ¢ç´¢ã€‚
åˆ‡æ›æ©Ÿåˆ¶é€šééš¨æ©Ÿæ¦‚ç‡

ğ‘

switch

p

switch

â€‹

å’Œå›ºå®šæ™‚é•·

ğ»

ğ·

ğ¸

ğ‘ƒ

H

DEP

â€‹

æ§åˆ¶ã€‚