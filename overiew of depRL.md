---
bg: "[[NTKLab_white bg.png]]"
---

<style>
    .reveal {
        font-family: 'Times New Roman', '標楷體';
        font-size: 30px;
        text-align: left;
        color: black;
        background-size: cover;
        background-position: center;
    }
	.reveal h1,
	.reveal h2,
	.reveal h3,
	.reveal h4,
	.reveal h5,
	.reveal h6 {
	  font-family: 'Times New Roman', '標楷體';
	  color: black;
	  %%text-transform: lowercase%%;
	  text-transform: capitalize;
	}
	.with-border{
		border: 1px solid red;
	}
</style>
<grid drag="60 10" drop="-3 40">
RL utility on Muscleskeleton modeling
<!-- element style="font-size: 35px;align: left; text-align: left;color: white"-->
</grid>

<grid drag="50 10" drop="40 70">
賴宏達、劉智翔
<!-- element style="font-size: 40px;align: right; text-align: right"-->
</grid>

<!-- slide bg="../NTKLab_white bg_cover_resize.png"-->

---
[[Natural and Robust Walking using Reinforcement Learning without Demonstrations in High-Dimensional Musculoskeletal Models.pdf]]
[[DEP-RL EMBODIED EXPLORATION FOR REINFORCEMENT LEARNING IN OVERACTUATED AND MUSCULOSKELETAL SYSTEMS.pdf]]
# Background
training file
settings file
desired algorithm, environment, number of training iterations

## Terms
- embodiment：實現部件
- Differential Extrinsic Plasticity, DEP：自組織並產生連貫的行為
- evolutionary priors：內建在生物體內的機制或目標

D4PG對於單一任務的多DOF動作應該足夠

---
## primary challenge
肌肉本身的化學低通濾波特性，單次短暫的肌肉收縮所產生的力矩，通常不足以在關節層面產生足夠的運動
>***"torques generated by short muscle twitches are often not sufficient to induce adequate motions on the joint level due to chemical low-pass filter properties"***

### Abstraction of muscle control
- motion primitives: 透過預處理，將訊號降維，extraction of muscle synergies
- 關節力矩 (joint torques) 轉換為肌肉刺激 (muscle stimulations) 的映射關係

從基本生理機制設計獎勵函數：
- 行走速度
- 關節疼痛
- 肌肉努力

---
## Challenge to the traditional RL
使用RL methods，需要注意：
- 過度驅動系統 (Overactuated Systems): 生物肌肉骨骼系統通常是過度驅動的，即肌肉數量多於自由度。在這種情況下，高效的探索策略是至關重要的。
- 動作空間 (Action Space): 肌肉控制任務的動作空間通常很大，因為需要控制許多單獨的肌肉
- ε-greedy 策略或零均值不相關高斯雜訊不夠用
- Ornstein-Uhlenbeck 雜訊。對於跨動作相關性的從頭開始學習任務不夠
- 難以通過簡單的試錯法探索

一樣要先擬合出這個人的肌肉狀態(要怎麼導入opensim資料)，然後再透過RL來應對跌倒之類的問題
- 明確的獎勵塑形
- 示範學習
- 特定訓練課程與檢查點

---
# Literature Review
- 降低肌肉維度
	- Tieck et al., 2018
	- Tahami et al., 2014
	- Crowder et al., 2021
- 手動分組肌肉以簡化學習
	- Joos et al., 2020
- 透軌跡最佳化控制 (Al Borno et al., 2020)
- 使用動作捕捉數據 (Lee et al., 2019)
- 在真實的肌肉機器人上產生動作 (Driess et al., 2018; Buchler et al., 2016)
- 明確的獎勵塑形、示範學習、特定訓練trials與檢查點NeurIPS
- Large Action Spaces
	- Hypergraph-based Architecture:Tavakoli et al., 2021
	- 離散動作空間 Dulac-Arnold et al., 2016; Wang et al., 2016
	- 行動先驗 (Action Priors) Biza et al., 2021; Singh et al., 2021專家數據
- 行動空間簡化與肌肉控制
	- PD控制器Luo et al., 2021
	- 校正網路限制力矩致動器的輸出Jiang et al., 2019
	- PCA降維：Al Borno et al., 2020; Zhao et al., 2022

---
## Hebbian learning
HL：活躍，增強連結(感測器和致動器的控制網路)，內部而非外部互動
***"Initial exploration As DEP is creating exploration that excites the system into various modes, we suggest running an unsupervised pre-training phase with exclusive DEP control."***
控制器動作
$$a_t = \tanh(\kappa C s_t + h_t)$$
- C：控制矩陣，狀態與動作間的mapping
- h：作為基準值，(joint limit) $\dot{h} \propto -a_t$
- 透過此凸顯連結$$C_{ij} = a_{i,\ t} \cdot s_{j,\ t}$$
- 差分HL$$C_{ij} = a_{i,\ t} \cdot \Delta s_{j,\ t}$$
以上觀測範圍太小了。
## Differential Extrinsic Plasticity(a learning rule)
DEP加入了長期環境回饋，即使用狀態速度(狀態的時變量)，用於加強狀態空間中連貫運動的感測器和動作之間的連接。

$$\tau \dot{C} = f(\dot{s}_t) \dot{s}_{t-\Delta t}^T - C$$
>recall：一階衰減項，隨時間逐漸減弱
>inverse prediction model$f(a)$可簡化為a
>可以假設感測器（sensor）與執行器（actuator）近似線性等之類的

$$f(\dot{s}_t) = M \cdot (\dot{s}_t - \dot{s}_{t-1})$$通過M將狀態變化轉換為執行器響應。

狀態空間的活動性
$$\tilde{C}_{ij} = \frac{C_{ij}}{\|C_{ij}\|_i + \epsilon}$$

### 缺點與解決方法
- perturbations sensitive
- behavioral deprivation: 侷限在特定模式
在高維中充分stochasticity可以避免
- DEP 與 RL 結合的優勢：強化學習策略可以干預 DEP，避免其陷入特定的極限週期，增強學習的靈活性。
- DEP 的直覺：DEP 能夠快速建立狀態與動作之間的高度相關性，實現系統的協調運動。

---
# Workflow
1. 初始探索（Initial Exploration）：
先由 DEP 進行完全控制的探索，收集大量非監督數據，初始化緩衝區。

2. 交替控制（Intra-Episode Exploration）：
	- 在每個 episode 中，DEP 和 RL 策略交替控制系統：
		- RL 策略負責目標導向的行為。
		- DEP 策略負責隨機探索。
		>切換機制通過隨機概率$p_{switch}$、固定時常$H_{DEP}$

挑戰：action結果如何結合?

## Dep class
我希望在預訓練時間使用外部資料，我有motion capture得到的關節座標點marker.trc、marker_IK、以及透過EMG資料、TendonLength、Joint Moment arm、_inverse_dynamics逆算優化所得到的Activations.sto, AdjustedEmgs.sto, FibreLengths.sto, FibreLengths.sto, FibreLengths.sto, MusclesContribution.sto, NormFibreLengths.sto, NormFibreVelocities.sto, ObjectiveFunctionComponentsAndWeightings.sto, Torques.sto

```
self.num_sensors = action_space.shape[0]
self.num_motors = action_space.shape[0]
```

reset會根據當前觀測空間形狀 (`obs_shape`)，調整控制器的維度。
rolling average
`_q_norm`
- l2
- max
- none

`_learn_controller`
- independent
- none
- global

---
## Gym封裝
透過`gymnasium.envs.registration.register`在Gym中引入`GaitGym`(其中有scone)
Wrapper的封裝可以應對內部參數的變動
也許需要透過
```python
import sconegym.gaitgym 
import deprl 
env = deprl.environments.Gym('sconewalk_h0918-v1', scaled_actions=False)
```
或者
```python
import sconegym
import gymnasium as gym

env = gym.make('sconewalk_h0918-v1')

```

 `python -m deprl.play --path .\baselines_DEPRL\sconewalk_h0918\ --num_episodes 100`
```python
path = r"D:\depRL\baselines_DEPRL\sconewalk_h0918\\"
checkpoint_file = None
checkpoint = "last"
seed = 0
num_episodes = 100
noisy = False
no_render = False
header = None
agent = None
environment = None
```

`environment.smooth_coeff`
沒有用到mpo_args?

```python
from deprl.custom_agents import dep_factory
from deprl.custom_mpo_torch import TunedMPO
from deprl.custom_replay_buffers import AdaptiveEnergyBuffer

# 創建 DEP 代理
agent = dep_factory(
    3,  # 集成的策略數量
    TunedMPO()  # 使用 MPO 演算法的調整版本
)(
    replay=AdaptiveEnergyBuffer(
        return_steps=1,
        batch_size=256,
        steps_between_batches=1000,
        batch_iterations=30,
        steps_before_batches=2e5,
        num_acts=18
    )
)

# 開始在 SconeGym 中進行訓練或測試
environment = deprl.environments.Gym('sconewalk_h0918-v1', scaled_actions=False)
agent.train(environment)
```

## Agent
MPO（Maximum a Posteriori Policy Optimization）
KL 散度約束策略更新，最大化後驗 (Posterior) 目標函式

[[depRL weights]]

(2, 18)
這怎麼做到的?根本沒有`deprl.environments.py`
`deprl.environments.Gym('sconewalk_h0918-v1', scaled_actions=False)`
`name = config["tonic"]["name"]`


這個是dep, `print(policy.expl)`
這個是RL, `print(policy.model)`

(temp) PS D:\depRL> & C:/ProgramData/anaconda3/Library/envs/temp/python.exe d:/depRL/sconegym/example_deprl.py
MyoSuite:> Registering Myo Envs
22:50:37 Successfully initialized OpenSim3 version 3.3-2021-01-28
22:50:37 Successfully initialized OpenSim4 version 4.4
22:50:37 Loaded settings from C:/Users/ed2di/AppData/Local/SCONE/scone-settings.zml
22:50:37 Successfully initialized Hyfydy version 1.8.3.1183 
22:50:37 Successfully initialized Hyfydy version 1.8.3.1183 Double Precision
Load SconeRun H2190 Baseline
Loading experiment from ./baselines_DEPRL/sconerun_h2190/checkpoints
Found only the policy checkpoint, the previous run was likely only run with  <'full_save': False>
Only loading policy checkpoint.
Stochastic Switch-DEP. Paper version.

Loading weights from ./baselines_DEPRL/sconerun_h2190/checkpoints\step_10000000.pt
Episode 0 ending; steps=83; reward=382.729;                 com=[ 0.905017 0.669366 -0.241684 ]
Episode 1 ending; steps=51; reward=351.834;                 com=[ 0.371481 0.755599 -0.127943 ]
Episode 2 ending; steps=68; reward=470.270;                 com=[ 0.625653 0.750073 0.083601 ]
Episode 3 ending; steps=91; reward=328.113;                 com=[ 0.922103 0.638765 0.005906 ]
Episode 4 ending; steps=56; reward=439.620;                 com=[ 0.630548 0.734331 0.187705 ]