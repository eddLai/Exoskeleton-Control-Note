---
bg: "[[NTKLab_white bg.png]]"
---

<style>
    .reveal {
        font-family: 'Times New Roman', '標楷體';
        font-size: 30px;
        text-align: left;
        color: black;
        background-size: cover;
        background-position: center;
    }
	.reveal h1,
	.reveal h2,
	.reveal h3,
	.reveal h4,
	.reveal h5,
	.reveal h6 {
	  font-family: 'Times New Roman', '標楷體';
	  color: black;
	  %%text-transform: lowercase%%;
	  text-transform: capitalize;
	}
	.with-border{
		border: 1px solid red;
	}
</style>
<grid drag="60 10" drop="-3 40">
RL utility on Muscleskeleton modeling
<!-- element style="font-size: 35px;align: left; text-align: left;color: white"-->
</grid>

<grid drag="50 10" drop="40 70">
賴宏達、劉智翔
<!-- element style="font-size: 40px;align: right; text-align: right"-->
</grid>

<!-- slide bg="../NTKLab_white bg_cover_resize.png"-->

---
[[Natural and Robust Walking using Reinforcement Learning without Demonstrations in High-Dimensional Musculoskeletal Models.pdf]]
[[DEP-RL EMBODIED EXPLORATION FOR REINFORCEMENT LEARNING IN OVERACTUATED AND MUSCULOSKELETAL SYSTEMS.pdf]]
# Background
training file
settings file
desired algorithm, environment, number of training iterations

## Terms
- embodiment：實現部件
- Differential Extrinsic Plasticity, DEP：自組織並產生連貫的行為
- evolutionary priors：內建在生物體內的機制或目標

D4PG對於單一任務的多DOF動作應該足夠

---
## primary challenge
肌肉本身的化學低通濾波特性，單次短暫的肌肉收縮所產生的力矩，通常不足以在關節層面產生足夠的運動
>***"torques generated by short muscle twitches are often not sufficient to induce adequate motions on the joint level due to chemical low-pass filter properties"***

### Abstraction of muscle control
- motion primitives: 透過預處理，將訊號降維，extraction of muscle synergies
- 關節力矩 (joint torques) 轉換為肌肉刺激 (muscle stimulations) 的映射關係

從基本生理機制設計獎勵函數：
- 行走速度
- 關節疼痛
- 肌肉努力

---
## Comparison to the origin RL
使用RL methods，需要注意：
- 過度驅動系統 (Overactuated Systems): 生物肌肉骨骼系統通常是過度驅動的，即肌肉數量多於自由度。在這種情況下，高效的探索策略是至關重要的。
- 動作空間 (Action Space): 肌肉控制任務的動作空間通常很大，因為需要控制許多單獨的肌肉
- ε-greedy 策略或零均值不相關高斯雜訊不夠用
- Ornstein-Uhlenbeck 雜訊。對於跨動作相關性的從頭開始學習任務不夠
- 難以通過簡單的試錯法探索

一樣要先擬合出這個人的肌肉狀態(要怎麼導入opensim資料)，然後再透過RL來應對跌倒之類的問題
- 明確的獎勵塑形
- 示範學習
- 特定訓練課程與檢查點

---
# Literature Review
- 降低肌肉維度
	- Tieck et al., 2018
	- Tahami et al., 2014
	- Crowder et al., 2021
- 手動分組肌肉以簡化學習
	- Joos et al., 2020
- 透軌跡最佳化控制 (Al Borno et al., 2020)
- 使用動作捕捉數據 (Lee et al., 2019)
- 在真實的肌肉機器人上產生動作 (Driess et al., 2018; Buchler et al., 2016)
- 明確的獎勵塑形、示範學習、特定訓練trials與檢查點NeurIPS
- Large Action Spaces
	- Hypergraph-based Architecture:Tavakoli et al., 2021
	- 離散動作空間 Dulac-Arnold et al., 2016; Wang et al., 2016
	- 行動先驗 (Action Priors) Biza et al., 2021; Singh et al., 2021專家數據
- 行動空間簡化與肌肉控制
	- PD控制器Luo et al., 2021
	- 校正網路限制力矩致動器的輸出Jiang et al., 2019
	- PCA降維：Al Borno et al., 2020; Zhao et al., 2022

---
- Hebbian learning, HL：活躍，增強連結(感測器和致動器的控制網路)，內部而非外部互動
- Differential Extrinsic Plasticity(a learning rule), DEP加入了環境回饋，用於加強狀態空間中連貫運動的感測器和動作之間的連接。
***"Initial exploration As DEP is creating exploration that excites the system into various modes, we suggest running an unsupervised pre-training phase with exclusive DEP control."***
控制器動作
$$a_t = \tanh(\kappa C s_t + h_t)$$
- C：控制矩陣，狀態與動作間的mapping
- h：作為基準值
- 透過$$\{}$$

$$\tau \dot{C} = f(\dot{s}_t) \dot{s}_{t-\Delta t}^T - C$$
狀態空間的活動性
$$\tilde{C}_{ij} = \frac{C_{ij}}{\|C_{ij}\|_i + \epsilon}
$$
