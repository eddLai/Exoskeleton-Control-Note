---
bg: "[[NTKLab_white bg.png]]"
---

<style>
    .reveal {
        font-family: 'Times New Roman', '標楷體';
        font-size: 30px;
        text-align: left;
        color: black;
        background-size: cover;
        background-position: center;
    }
	.reveal h1,
	.reveal h2,
	.reveal h3,
	.reveal h4,
	.reveal h5,
	.reveal h6 {
	  font-family: 'Times New Roman', '標楷體';
	  color: black;
	  %%text-transform: lowercase%%;
	  text-transform: capitalize;
	}
	.with-border{
		border: 1px solid red;
	}
</style>
<grid drag="60 10" drop="-3 40">
RL utility on Muscleskeleton modeling
<!-- element style="font-size: 35px;align: left; text-align: left;color: white"-->
</grid>

<grid drag="50 10" drop="40 70">
賴宏達、劉智翔
<!-- element style="font-size: 40px;align: right; text-align: right"-->
</grid>

<!-- slide bg="../NTKLab_white bg_cover_resize.png"-->

---
[[Natural and Robust Walking using Reinforcement Learning without Demonstrations in High-Dimensional Musculoskeletal Models.pdf]]
[[DEP-RL EMBODIED EXPLORATION FOR REINFORCEMENT LEARNING IN OVERACTUATED AND MUSCULOSKELETAL SYSTEMS.pdf]]
# Background
training file
settings file
desired algorithm, environment, number of training iterations

## Terms
- embodiment：實現部件
- Differential Extrinsic Plasticity, DEP：自組織並產生連貫的行為
- evolutionary priors：內建在生物體內的機制或目標

D4PG對於單一任務的多DOF動作應該足夠

---
## primary challenge
肌肉本身的化學低通濾波特性，單次短暫的肌肉收縮所產生的力矩，通常不足以在關節層面產生足夠的運動
>***"torques generated by short muscle twitches are often not sufficient to induce adequate motions on the joint level due to chemical low-pass filter properties"***

### Abstraction of muscle control
- motion primitives: 透過預處理，將訊號降維，extraction of muscle synergies
- 關節力矩 (joint torques) 轉換為肌肉刺激 (muscle stimulations) 的映射關係

從基本生理機制設計獎勵函數：
- 行走速度
- 關節疼痛
- 肌肉努力

---
## Challenge to the traditional RL
使用RL methods，需要注意：
- 過度驅動系統 (Overactuated Systems): 生物肌肉骨骼系統通常是過度驅動的，即肌肉數量多於自由度。在這種情況下，高效的探索策略是至關重要的。
- 動作空間 (Action Space): 肌肉控制任務的動作空間通常很大，因為需要控制許多單獨的肌肉
- ε-greedy 策略或零均值不相關高斯雜訊不夠用
- Ornstein-Uhlenbeck 雜訊。對於跨動作相關性的從頭開始學習任務不夠
- 難以通過簡單的試錯法探索

一樣要先擬合出這個人的肌肉狀態(要怎麼導入opensim資料)，然後再透過RL來應對跌倒之類的問題
- 明確的獎勵塑形
- 示範學習
- 特定訓練課程與檢查點

---
# Literature Review
- 降低肌肉維度
	- Tieck et al., 2018
	- Tahami et al., 2014
	- Crowder et al., 2021
- 手動分組肌肉以簡化學習
	- Joos et al., 2020
- 透軌跡最佳化控制 (Al Borno et al., 2020)
- 使用動作捕捉數據 (Lee et al., 2019)
- 在真實的肌肉機器人上產生動作 (Driess et al., 2018; Buchler et al., 2016)
- 明確的獎勵塑形、示範學習、特定訓練trials與檢查點NeurIPS
- Large Action Spaces
	- Hypergraph-based Architecture:Tavakoli et al., 2021
	- 離散動作空間 Dulac-Arnold et al., 2016; Wang et al., 2016
	- 行動先驗 (Action Priors) Biza et al., 2021; Singh et al., 2021專家數據
- 行動空間簡化與肌肉控制
	- PD控制器Luo et al., 2021
	- 校正網路限制力矩致動器的輸出Jiang et al., 2019
	- PCA降維：Al Borno et al., 2020; Zhao et al., 2022

---
## Hebbian learning
HL：活躍，增強連結(感測器和致動器的控制網路)，內部而非外部互動
***"Initial exploration As DEP is creating exploration that excites the system into various modes, we suggest running an unsupervised pre-training phase with exclusive DEP control."***
控制器動作
$$a_t = \tanh(\kappa C s_t + h_t)$$
- C：控制矩陣，狀態與動作間的mapping
- h：作為基準值，(joint limit) $\dot{h} \propto -a_t$
- 透過此凸顯連結$$C_{ij} = a_{i,\ t} \cdot s_{j,\ t}$$
- 差分HL$$C_{ij} = a_{i,\ t} \cdot \Delta s_{j,\ t}$$
以上觀測範圍太小了。
## Differential Extrinsic Plasticity(a learning rule)
DEP加入了長期環境回饋，即使用狀態速度(狀態的時變量)，用於加強狀態空間中連貫運動的感測器和動作之間的連接。

$$\tau \dot{C} = f(\dot{s}_t) \dot{s}_{t-\Delta t}^T - C$$
>recall：一階衰減項，隨時間逐漸減弱
>inverse prediction model$f(a)$可簡化為a
>可以假設感測器（sensor）與執行器（actuator）近似線性等之類的

$$f(\dot{s}_t) = M \cdot (\dot{s}_t - \dot{s}_{t-1})$$通過M將狀態變化轉換為執行器響應。

狀態空間的活動性
$$\tilde{C}_{ij} = \frac{C_{ij}}{\|C_{ij}\|_i + \epsilon}$$

### 缺點與解決方法
- perturbations sensitive
- behavioral deprivation: 侷限在特定模式
在高維中充分stochasticity可以避免
- DEP 與 RL 結合的優勢：強化學習策略可以干預 DEP，避免其陷入特定的極限週期，增強學習的靈活性。
- DEP 的直覺：DEP 能夠快速建立狀態與動作之間的高度相關性，實現系統的協調運動。

---
# Workflow
1. 初始探索（Initial Exploration）：
先由 DEP 進行完全控制的探索，收集大量非監督數據，初始化緩衝區。

2. 交替控制（Intra-Episode Exploration）：
	- 在每個 episode 中，DEP 和 RL 策略交替控制系統：
		- RL 策略負責目標導向的行為。
		- DEP 策略負責隨機探索。
		>切換機制通過隨機概率$p_{switch}$、固定時常$H_{DEP}$

挑戰：action結果如何結合?

## Dep class
我希望在預訓練時間使用外部資料，我有motion capture得到的關節座標點marker.trc、marker_IK、以及透過EMG資料、TendonLength、Joint Moment arm、_inverse_dynamics逆算優化所得到的Activations.sto, AdjustedEmgs.sto, FibreLengths.sto, FibreLengths.sto, FibreLengths.sto, MusclesContribution.sto, NormFibreLengths.sto, NormFibreVelocities.sto, ObjectiveFunctionComponentsAndWeightings.sto, Torques.sto

```
self.num_sensors = action_space.shape[0]
self.num_motors = action_space.shape[0]
```

reset會根據當前觀測空間形狀 (`obs_shape`)，調整控制器的維度。
rolling average
`_q_norm`
- l2
- max
- none

`_learn_controller`
- independent
- none
- global

---
## Gym封裝
透過`gymnasium.envs.registration.register`在Gym中引入`GaitGym`(其中有scone)
Wrapper的封裝可以應對內部參數的變動
也許需要透過
```python
import sconegym.gaitgym 
import deprl 
env = deprl.environments.Gym('sconewalk_h0918-v1', scaled_actions=False)
```
或者
```python
import sconegym
import gymnasium as gym

env = gym.make('sconewalk_h0918-v1')

```