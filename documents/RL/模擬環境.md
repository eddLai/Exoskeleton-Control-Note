```python
import gym
import ptan
import numpy as np

class RobotEnv(gym.Env):
    def __init__(self):
        super(RobotEnv, self).__init__()
        # 定義觀測空間和行動空間
        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32)
        self.action_space = gym.spaces.Discrete(n=...)  # 根據您的機器人定義行動數量

    def reset(self):
        # 重置環境並返回初始觀測
        return self._get_observation()

    def step(self, action):
        # 執行行動，更新環境狀態
        # 假設 execute_action 是您實現來控制機器人的方法
        self.execute_action(action)
        observation = self._get_observation()
        reward = ...  # 計算獎勵
        done = ...  # 判斷是否結束
        return observation, reward, done, {}

    def _get_observation(self):
        # 從機器人獲取當前的觀測數據
        # 假設 get_imu_data 返回角度、角速度和角加速度
        return np.array(self.get_imu_data())

# 然後，創建您的環境和代理實例
env = RobotEnv()
agent = ...  # 這裡應該是您定義的代理，它需要與PTAN兼容

# 使用PTAN的ExperienceSource
exp_source = ptan.experience.ExperienceSource(env=env, agent=agent, steps_count=4)

```
